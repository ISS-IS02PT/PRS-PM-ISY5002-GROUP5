{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parkway Project Use Case 1: Write Off Cases Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Users\\mokky\\Documents\\GitHub\\nus-iss\\PRS-PM-ISY5002-GROUP5\\SystemCode\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('d:/Users/mokky/Documents/GitHub/nus-iss/PRS-PM-ISY5002-GROUP5/SystemCode')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datapipeline import Datapipeline\n",
    "dpl = Datapipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = './data/uc1'\n",
    "dict_X_train_file_paths = {\n",
    "#    'GHL' : [file_dir + 'GHL_new_train_X_uc1.pkl'],\n",
    "    'MEH' : [file_dir + 'MEH_new_train_X_uc1.pkl'],\n",
    "#    'PEH' : [file_dir + 'PEH_new_train_X_uc1.pkl'],\n",
    "#    'PNH' : [file_dir + 'PNH_new_train_X_uc1.pkl']\n",
    "}\n",
    "dict_y_train_file_paths = {\n",
    "    'GHL' : file_dir + 'GHL_data_y_train.pkl',\n",
    "    'MEH' : file_dir + 'MEH_data_y_train.pkl',\n",
    "    'PEH' : file_dir + 'PEH_data_y_train.pkl',\n",
    "    'PNH' : file_dir + 'PNH_data_y_train.pkl'\n",
    "}\n",
    "dict_X_test_file_paths = {\n",
    "    'GHL' : [file_dir + 'GHL_new_test_X_uc1.pkl'],\n",
    "    'MEH' : [file_dir + 'MEH_new_test_X_uc1.pkl'],\n",
    "    'PEH' : [file_dir + 'PEH_new_test_X_uc1.pkl'],\n",
    "    'PNH' : [file_dir + 'PNH_new_test_X_uc1.pkl']\n",
    "}\n",
    "dict_y_test_file_paths = {\n",
    "    'GHL' : file_dir + 'GHL_data_y_test.pkl',\n",
    "    'MEH' : file_dir + 'MEH_data_y_test.pkl',\n",
    "    'PEH' : file_dir + 'PEH_data_y_test.pkl',\n",
    "    'PNH' : file_dir + 'PNH_data_y_test.pkl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train PEH (28604, 38885)\n",
      "y_train PEH (28604,)\n",
      "X_test PEH (9534, 38885)\n",
      "y_test PEH (9534,)\n"
     ]
    }
   ],
   "source": [
    "dict_df_X_train = {}\n",
    "dict_df_y_train = {}\n",
    "dict_df_X_test = {}\n",
    "dict_df_y_test = {}\n",
    "\n",
    "for hosp in dict_X_train_file_paths:\n",
    "    dict_df_X_train[hosp] = pd.concat([pd.read_pickle(file_path)\n",
    "                                       for file_path in dict_X_train_file_paths[hosp]])\n",
    "    print(f'X_train {hosp} {dict_df_X_train[hosp].shape}')\n",
    "    \n",
    "    dict_df_y_train[hosp] = pd.read_pickle(dict_y_train_file_paths[hosp])\n",
    "    print(f'y_train {hosp} {dict_df_y_train[hosp].shape}')\n",
    "\n",
    "    dict_df_X_test[hosp] = pd.concat([pd.read_pickle(file_path)\n",
    "                                      for file_path in dict_X_test_file_paths[hosp]])\n",
    "    print(f'X_test {hosp} {dict_df_X_test[hosp].shape}')\n",
    "\n",
    "    dict_df_y_test[hosp] = pd.read_pickle(dict_y_test_file_paths[hosp])\n",
    "    print(f'y_test {hosp} {dict_df_y_test[hosp].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(df):\n",
    "    df1 = dpl.bin_column(df.to_frame(), col='WRITE_OFF', bin_thresh = [500])\n",
    "    df1 = pd.get_dummies(df1['bin_WRITE_OFF'])\n",
    "    df1 = df1.drop(0, axis=1)\n",
    "    df1.columns = ['WRITE_OFF_LABEL']\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hosp in dict_df_y_train:\n",
    "    dict_df_y_train[hosp] = convert_y(dict_df_y_train[hosp])\n",
    "    dict_df_y_test[hosp] = convert_y(dict_df_y_test[hosp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree WITHOUT SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = 'entropy'\n",
    "rand_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
      "Accuracy on PEH training set without SMOTE decision tree: 1.0\n",
      "Accuracy on PEH test set without SMOTE decision tree: 0.998846234529054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\ipykernel_launcher.py:9: ResourceWarning: unclosed file <_io.BufferedWriter name='./Data Exploration/data/PEH_uc1_no_smote_dt_model.pkl'>\n",
      "  if __name__ == '__main__':\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEH decision tree\n",
      " [[9523    4]\n",
      " [   7    0]]\n",
      "PEH decision tree\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9527\n",
      "           1       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           1.00      9534\n",
      "   macro avg       0.50      0.50      0.50      9534\n",
      "weighted avg       1.00      1.00      1.00      9534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for hosp in dict_df_X_train:\n",
    "    model = DecisionTreeClassifier(criterion=criterion,random_state=rand_seed)\n",
    "    model.fit(dict_df_X_train[hosp], dict_df_y_train[hosp])\n",
    "    print(model)\n",
    "    print(f\"Accuracy on {hosp} training set without SMOTE decision tree: {model.score(dict_df_X_train[hosp], dict_df_y_train[hosp])}\")\n",
    "    print(f\"Accuracy on {hosp} test set without SMOTE decision tree: {model.score(dict_df_X_test[hosp], dict_df_y_test[hosp])}\")\n",
    "    \n",
    "    filename = file_dir + f'{hosp}_uc1_no_smote_dt_model.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "    y_pred = model.predict(dict_df_X_test[hosp])\n",
    "    print(hosp, 'decision tree\\n', confusion_matrix(dict_df_y_test[hosp], y_pred))  \n",
    "    print(hosp, 'decision tree\\n', classification_report(dict_df_y_test[hosp], y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree WITH SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree WITH SMOTE SS = 0.1 ~ 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
      "Accuracy on PEH training set with 0.1 SMOTE decision tree: 1.0\n",
      "Accuracy on PEH test set with 0.1 SMOTE decision tree: 0.9982169079085379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\ipykernel_launcher.py:13: ResourceWarning: unclosed file <_io.BufferedWriter name='./Data Exploration/data/PEH_uc1_0.1_smote_dt_model.pkl'>\n",
      "  del sys.path[0]\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEH decision tree \n",
      " [[9517   10]\n",
      " [   7    0]]\n",
      "PEH decision tree \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9527\n",
      "           1       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           1.00      9534\n",
      "   macro avg       0.50      0.50      0.50      9534\n",
      "weighted avg       1.00      1.00      1.00      9534\n",
      "\n",
      "DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
      "Accuracy on PEH training set with 0.2 SMOTE decision tree: 1.0\n",
      "Accuracy on PEH test set with 0.2 SMOTE decision tree: 0.9983217956786239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\ipykernel_launcher.py:13: ResourceWarning: unclosed file <_io.BufferedWriter name='./Data Exploration/data/PEH_uc1_0.2_smote_dt_model.pkl'>\n",
      "  del sys.path[0]\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEH decision tree \n",
      " [[9518    9]\n",
      " [   7    0]]\n",
      "PEH decision tree \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9527\n",
      "           1       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           1.00      9534\n",
      "   macro avg       0.50      0.50      0.50      9534\n",
      "weighted avg       1.00      1.00      1.00      9534\n",
      "\n",
      "DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
      "Accuracy on PEH training set with 0.30000000000000004 SMOTE decision tree: 1.0\n",
      "Accuracy on PEH test set with 0.30000000000000004 SMOTE decision tree: 0.9984266834487099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\ipykernel_launcher.py:13: ResourceWarning: unclosed file <_io.BufferedWriter name='./Data Exploration/data/PEH_uc1_0.30000000000000004_smote_dt_model.pkl'>\n",
      "  del sys.path[0]\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEH decision tree \n",
      " [[9519    8]\n",
      " [   7    0]]\n",
      "PEH decision tree \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9527\n",
      "           1       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           1.00      9534\n",
      "   macro avg       0.50      0.50      0.50      9534\n",
      "weighted avg       1.00      1.00      1.00      9534\n",
      "\n",
      "DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
      "Accuracy on PEH training set with 0.4 SMOTE decision tree: 1.0\n",
      "Accuracy on PEH test set with 0.4 SMOTE decision tree: 0.9984266834487099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\ipykernel_launcher.py:13: ResourceWarning: unclosed file <_io.BufferedWriter name='./Data Exploration/data/PEH_uc1_0.4_smote_dt_model.pkl'>\n",
      "  del sys.path[0]\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEH decision tree \n",
      " [[9519    8]\n",
      " [   7    0]]\n",
      "PEH decision tree \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9527\n",
      "           1       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           1.00      9534\n",
      "   macro avg       0.50      0.50      0.50      9534\n",
      "weighted avg       1.00      1.00      1.00      9534\n",
      "\n",
      "DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
      "Accuracy on PEH training set with 0.5 SMOTE decision tree: 1.0\n",
      "Accuracy on PEH test set with 0.5 SMOTE decision tree: 0.9977973568281938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\ipykernel_launcher.py:13: ResourceWarning: unclosed file <_io.BufferedWriter name='./Data Exploration/data/PEH_uc1_0.5_smote_dt_model.pkl'>\n",
      "  del sys.path[0]\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEH decision tree \n",
      " [[9513   14]\n",
      " [   7    0]]\n",
      "PEH decision tree \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9527\n",
      "           1       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           1.00      9534\n",
      "   macro avg       0.50      0.50      0.50      9534\n",
      "weighted avg       1.00      1.00      1.00      9534\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 13.2 GiB for an array with shape (38885, 45731) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-eb4b36e31e86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msm_ss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m55\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mX_train_ss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_ss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm_ss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_df_X_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhosp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_df_y_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhosp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrand_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     86\u001b[0m               if binarize_y else output[1])\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mX_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrays_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\imblearn\\utils\\_validation.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfrom_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_props\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfrom_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_props\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\imblearn\\utils\\_validation.py\u001b[0m in \u001b[0;36m_transfrom_one\u001b[1;34m(self, array, props)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprops\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprops\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dtypes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"series\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   5552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5553\u001b[0m         \u001b[1;31m# GH 19920: retain column metadata after concat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5554\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5555\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5556\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    285\u001b[0m     )\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m                 \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"concat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1683\u001b[1;33m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1684\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1685\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mform_blocks\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     \u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1740\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FloatBlock\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1741\u001b[1;33m         \u001b[0mfloat_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_multi_blockify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FloatBlock\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1742\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_multi_blockify\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup_block\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1835\u001b[1;33m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stack_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup_block\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1837\u001b[0m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_stack_arrays\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0m_shape_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m     \u001b[0mstacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1864\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m         \u001b[0mstacked\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 13.2 GiB for an array with shape (38885, 45731) and data type float64"
     ]
    }
   ],
   "source": [
    "for hosp in dict_df_X_train:\n",
    "    for sample in np.arange(0.1, 1.1, 0.1):\n",
    "        sm_ss = SMOTE(random_state=55,sampling_strategy=sample)\n",
    "        X_train_ss, y_train_ss = sm_ss.fit_sample(dict_df_X_train[hosp], dict_df_y_train[hosp])\n",
    "        \n",
    "        model = DecisionTreeClassifier(criterion=criterion,random_state=rand_seed)\n",
    "        model.fit(X_train_ss, y_train_ss)\n",
    "        print(model)\n",
    "        print(f\"Accuracy on {hosp} training set with {sample} SMOTE decision tree: {model.score(X_train_ss, y_train_ss)}\")\n",
    "        print(f\"Accuracy on {hosp} test set with {sample} SMOTE decision tree: {model.score(dict_df_X_test[hosp], dict_df_y_test[hosp])}\")\n",
    "    \n",
    "        filename = file_dir + f'{hosp}_uc1_{sample}_smote_dt_model.pkl'\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "        \n",
    "        y_pred = model.predict(dict_df_X_test[hosp])\n",
    "        print(hosp, 'decision tree \\n', confusion_matrix(dict_df_y_test[hosp], y_pred))  \n",
    "        print(hosp, 'decision tree \\n', classification_report(dict_df_y_test[hosp], y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_strength = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hosp in dict_df_X_train:\n",
    "    model = LogisticRegression(C=reg_strength).fit(dict_df_X_train[hosp], dict_df_y_train[hosp])\n",
    "    print(model)\n",
    "    print(f\"{hosp} Training set without SMOTE log reg score: {model.score(dict_df_X_train[hosp], dict_df_y_train[hosp])}\")\n",
    "    print(f\"{hosp} Test set without SMOTE log reg score: {model.score(dict_df_X_test[hosp], dict_df_y_test[hosp])}\")\n",
    "    \n",
    "    filename = file_dir + f'{hosp}_uc1_no_smote_logreg_model.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "    y_pred = model.predict(dict_df_X_test[hosp])\n",
    "    print(hosp, 'log reg \\n', confusion_matrix(dict_df_y_test[hosp], y_pred))  \n",
    "    print(hosp, 'log reg \\n', classification_report(dict_df_y_test[hosp], y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression With SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression With SMOTE ss=0.1 ~ 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hosp in dict_df_X_train:\n",
    "    for sample in np.arange(0.1, 1.1, 0.1):\n",
    "        sm_ss = SMOTE(random_state=55,sampling_strategy=sample)\n",
    "        X_train_ss, y_train_ss = sm_ss.fit_sample(dict_df_X_train[hosp], dict_df_y_train[hosp])\n",
    "        \n",
    "        model = LogisticRegression(C=reg_strength).fit(X_train_ss, y_train_ss)\n",
    "        print(model)\n",
    "        print(f\"{hosp} Training set with {sample} SMOTE log reg score: {model.score(X_train_ss, y_train_ss)}\")\n",
    "        print(f\"{hosp} Test set with {sample} SMOTE log reg score: {model.score(dict_df_X_test[hosp], dict_df_y_test[hosp])}\")\n",
    "\n",
    "        filename = file_dir + f'{hosp}_uc1_{sample}_smote_logreg_model.pkl'\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "        y_pred = model.predict(dict_df_X_test[hosp])\n",
    "        print(hosp, 'log reg \\n', confusion_matrix(dict_df_y_test[hosp], y_pred))  \n",
    "        print(hosp, 'log reg \\n', classification_report(dict_df_y_test[hosp], y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hosp in dict_df_X_train:\n",
    "    # Initiating the Gaussian Classifier\n",
    "    model = GaussianNB()\n",
    "\n",
    "    # Training your model \n",
    "    model.fit(dict_df_X_train[hosp], dict_df_y_train[hosp])\n",
    "    print(model)\n",
    "    filename = file_dir + f'{hosp}_uc1_no_smote_nb_model.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "    # Score\n",
    "    print(f\"{hosp} Training set without SMOTE Naive Bayes score: {model.score(dict_df_X_train[hosp], dict_df_y_train[hosp])}\")\n",
    "    print(f\"{hosp} Test set without SMOTE Naive Bayes score: {model.score(dict_df_X_test[hosp], dict_df_y_test[hosp])}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    y_pred = model.predict(dict_df_X_test[hosp])\n",
    "    print(hosp, 'Naive Bayes \\n', confusion_matrix(dict_df_y_test[hosp], y_pred))  \n",
    "    print(hosp, 'Naive Bayes \\n', classification_report(dict_df_y_test[hosp], y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes With SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Without SMOTE ss=0.1 ~ 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hosp in dict_df_X_train:\n",
    "    for sample in np.arange(0.1, 1.1, 0.1):\n",
    "        sm_ss = SMOTE(random_state=55,sampling_strategy=sample)\n",
    "        X_train_ss, y_train_ss = sm_ss.fit_sample(dict_df_X_train[hosp], dict_df_y_train[hosp])\n",
    "        \n",
    "        # Initiating the Gaussian Classifier\n",
    "        model = GaussianNB()\n",
    "        \n",
    "        # Training your model \n",
    "        model.fit(X_train_ss, y_train_ss)\n",
    "        print(model)\n",
    "        filename = file_dir + f'{hosp}_uc1_{sample}_smote_nb_model.pkl'\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "        # Score\n",
    "        print(f\"{hosp} Training set with {sample} SMOTE Naive Bayes score: {model.score(X_train_ss, y_train_ss)}\")\n",
    "        print(f\"{hosp} Test set with {sample} SMOTE Naive Bayes score: {model.score(dict_df_X_test[hosp], dict_df_y_test[hosp])}\")\n",
    "\n",
    "        # Confusion Matrix\n",
    "        y_pred = model.predict(dict_df_X_test[hosp])\n",
    "        print(hosp, 'Naive Bayes\\n', confusion_matrix(dict_df_y_test[hosp], y_pred))  \n",
    "        print(hosp, 'Naive Bayes\\n', classification_report(dict_df_y_test[hosp], y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## NEURAL NET MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net MLP Without SMOTE, 2 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hid_layers = [(128,128), (16,16,16), (32,32,32), (16,16,16,16), (32,32,32,32)]\n",
    "max_iter = 1000\n",
    "list_smote_sampling = [0.1, 0.2, 0.3, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Net MLPClassifier without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hosp in dict_df_X_train:\n",
    "    for hid_layers in list_hid_layers:\n",
    "        model = MLPClassifier(hidden_layer_sizes=hid_layers, max_iter=max_iter,verbose=1)  \n",
    "        model.fit(dict_df_X_train[hosp], dict_df_y_train[hosp])\n",
    "        print(model)\n",
    "        filename = file_dir + f'{hosp}_uc1_no_smote_mlp_{hid_layers}_model.pkl'\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "        predictions = mlp.predict(dict_df_X_test[hosp]) \n",
    "\n",
    "        # Score\n",
    "        print(f\"{hosp} Training set without SMOTE mlp {hid_layers} score: {model.score(dict_df_X_train[hosp], dict_df_y_train[hosp])}\")\n",
    "        print(f\"{hosp} Test set without SMOTE mlp {hid_layers} score: {model.score(dict_df_X_test[hosp], dict_df_y_test[hosp])}\")\n",
    "\n",
    "        print(f\"{hosp} Accuracy without SMOTE mlp {hid_layers}: \", metrics.accuracy_score(dict_df_y_test[hosp], predictions))\n",
    "        print(hosp, f'mlp {hid_layers}\\n', confusion_matrix(dict_df_y_test[hosp],predictions))  \n",
    "        print(hosp, f'mlp {hid_layers}\\n', classification_report(dict_df_y_test[hosp],predictions))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hid_layers = [(16,16), (32,32), (64,64), (128,128), (256,256),\n",
    "                   (16,16,16), (32,32,32), (64,64,64),\n",
    "                   (16,16,16,16), (32,32,32,32)]\n",
    "max_iter = 1000\n",
    "list_smote_sampling = [0.1, 0.2, 0.3, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Net Layers With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hosp in dict_df_X_train:\n",
    "    for hid_layers in list_hid_layers:\n",
    "        for sample in list_smote_sampling:\n",
    "            sm_ss = SMOTE(random_state=55,sampling_strategy=sample)\n",
    "            X_train_ss, y_train_ss = sm_ss.fit_sample(dict_df_X_train[hosp], dict_df_y_train[hosp])\n",
    "        \n",
    "            model = MLPClassifier(hidden_layer_sizes=hid_layers, max_iter=max_iter,verbose=1)  \n",
    "            model.fit(X_train_ss, y_train_ss)  \n",
    "            print(model)\n",
    "            filename = file_dir + f'{hosp}_uc1_{sample}_smote_mlp_{hid_layers}_model.pkl'\n",
    "            pickle.dump(model, open(filename, 'wb'))\n",
    "            \n",
    "            predictions = model.predict(dict_df_X_test[hosp]) \n",
    "\n",
    "            # Score\n",
    "            print(f\"{hosp} Training set {sample} SMOTE mlp {hid_layers} score: {model.score(X_train_ss, y_train_ss)}\")\n",
    "            print(f\"{hosp} Test set {sample} SMOTE mlp {hid_layers} score: {model.score(dict_df_X_test[hosp], dict_df_y_test[hosp])}\")\n",
    "\n",
    "            print(f\"{hosp} Accuracy: \", metrics.accuracy_score(dict_df_y_test[hosp], predictions))\n",
    "            print(hosp, f'mlp {hid_layers}\\n', confusion_matrix(dict_df_y_test[hosp],predictions))  \n",
    "            print(hosp, f'mlp {hid_layers}\\n', classification_report(dict_df_y_test[hosp],predictions))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful when running this, about 2 full days are needed to run this!!!\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hosp in dict_df_X_train:\n",
    "    test_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    for i in range(1,15):\n",
    "\n",
    "        knn = KNeighborsClassifier(i)\n",
    "        knn.fit(dict_df_X_train[hosp],dict_df_y_train[hosp])\n",
    "\n",
    "        train_scores.append(knn.score(dict_df_X_train[hosp],dict_df_y_train[hosp]))\n",
    "        test_scores.append(knn.score(dict_df_X_test[hosp],dict_df_y_test[hosp]))\n",
    "\n",
    "    ## score that comes from testing on the same datapoints that were used for training\n",
    "    max_train_score = max(train_scores)\n",
    "    train_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\n",
    "    print(f'{hosp} Max train score {max_train_score*100} % and k = {list(map(lambda x: x+1, train_scores_ind))}')\n",
    "\n",
    "    ## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\n",
    "    max_test_score = max(test_scores)\n",
    "    test_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\n",
    "    print(f'{hosp} Max test score {max_test_score*100} % and k = {list(map(lambda x: x+1, test_scores_ind))}')\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(range(1,15),train_scores,marker='*',label='Train Score')\n",
    "    plt.plot(range(1,15),test_scores,marker='o',label='Test Score')\n",
    "    plt.title(f'{hosp} KNN')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# From the above, the best KNN is at k=8\n",
    "#Setup a knn classifier with k neighbors\n",
    "best_knn = KNeighborsClassifier(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbours = 8\n",
    "for hosp in dict_df_X_train:\n",
    "    # Fit the KNN model with training data and score the test data\n",
    "    best_knn = KNeighborsClassifier(n_neighbours)\n",
    "    \n",
    "    best_knn.fit(dict_df_X_train[hosp],dict_df_y_train[hosp])\n",
    "    best_knn.score(dict_df_X_test[hosp],dict_df_y_test[hosp])\n",
    "    \n",
    "    # save the model to disk\n",
    "    filename = f'{hosp}_best_knn_without_smote.pkl'\n",
    "    pickle.dump(best_knn, open(filename, 'wb'))\n",
    "    \n",
    "    y_pred = best_knn.predict(dict_df_X_test[hosp])\n",
    "    # Score for Best KNN, No SMOTE\n",
    "    print(f\"{hosp} Training set without SMOTE KNN({n_neighbours}) score: {best_knn.score(dict_df_X_train[hosp], dict_df_y_train[hosp])}\")\n",
    "    print(f\"{hosp} Test set without SMOTE KNN({n_neighbours}) score: {best_knn.score(dict_df_X_test[hosp], dict_df_y_test[hosp])}\")\n",
    "\n",
    "    print(f\"{hosp} Accuracy without SMOTE KNN({n_neighbours}): {metrics.accuracy_score(dict_df_y_test[hosp], y_pred)}\")\n",
    "    print(hosp, f'KNN({n_neighbours})\\n', confusion_matrix(dict_df_y_test[hosp],y_pred))  \n",
    "    print(hosp, f'KNN({n_neighbours})\\n', classification_report(dict_df_y_test[hosp],y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN With SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN With SMOTE ss=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbours = 8\n",
    "list_smote_sampling = [0.1, 0.5, 1.0]\n",
    "for hosp in dict_df_X_train:\n",
    "    for sample in list_smote_sampling:\n",
    "        sm_ss = SMOTE(random_state=55,sampling_strategy=sample)\n",
    "        X_train_ss, y_train_ss = sm_ss.fit_sample(dict_df_X_train[hosp], dict_df_y_train[hosp])\n",
    "        \n",
    "        # Fit the KNN model with training data and score the test data\n",
    "        best_knn = KNeighborsClassifier(n_neighbours)    \n",
    "        best_knn.fit(X_train_ss,y_train_ss)\n",
    "        #best_knn.score(dict_df_X_test[hosp],dict_df_y_test[hosp])\n",
    "        \n",
    "        # save the model to disk\n",
    "        filename = f'{hosp}_best_knn_{sample}_smote.pkl'\n",
    "        pickle.dump(best_knn, open(filename, 'wb'))\n",
    "        \n",
    "        y_pred = best_knn.predict(dict_df_X_test[hosp])\n",
    "        # Score for Best KNN, with SMOTE\n",
    "        print(f\"{hosp} Training set {sample} SMOTE KNN({n_neighbours}) score: {best_knn.score(X_train_ss, y_train_ss)}\")\n",
    "        print(f\"{hosp} Test set {sample} SMOTE KNN({n_neighbours}) score: {best_knn.score(dict_df_X_test[hosp], dict_df_y_test[hosp])}\")\n",
    "\n",
    "        print(f\"{hosp} Accuracy {sample} SMOTE KNN({n_neighbours}): \", metrics.accuracy_score(dict_df_y_test[hosp], y_pred))\n",
    "        print(confusion_matrix(dict_df_y_test[hosp],y_pred))  \n",
    "        print(classification_report(dict_df_y_test[hosp],y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parkway Project Use Case 1: Write Off Cases Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Users\\mokky\\Documents\\GitHub\\nus-iss\\PRS-PM-ISY5002-GROUP5\\SystemCode\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('d:/Users/mokky/Documents/GitHub/nus-iss/PRS-PM-ISY5002-GROUP5/SystemCode')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapipeline import Datapipeline\n",
    "dpl = Datapipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data Exploration/data/all_hosp_data.pkl\n"
     ]
    }
   ],
   "source": [
    "file_dir = './data/uc1/'\n",
    "raw_data_path = './Data Exploration/data/Latest_ParkwaySampleDataForProject_12_WithLabelNoFormula.xlsx'\n",
    "proc_data_path = dpl.transform_raw_data(raw_data_path, split_hosp=False)\n",
    "print(proc_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train X rows: 0 ~ 10000\n",
      "Processing train X rows: 10000 ~ 20000\n",
      "Processing train X rows: 20000 ~ 30000\n",
      "Processing train X rows: 30000 ~ 40000\n",
      "Processing train X rows: 40000 ~ 50000\n",
      "Processing train X rows: 50000 ~ 60000\n",
      "Processing train X rows: 60000 ~ 70000\n",
      "Processing train X rows: 70000 ~ 80000\n",
      "Processing train X rows: 80000 ~ 90000\n",
      "Processing train X rows: 90000 ~ 100000\n",
      "Processing train X rows: 100000 ~ 110000\n",
      "Processing train X rows: 110000 ~ 120000\n",
      "Processing train X rows: 120000 ~ 130000\n",
      "Processing train X rows: 130000 ~ 140000\n",
      "Processing train X rows: 140000 ~ 150000\n",
      "Processing train X rows: 150000 ~ 160000\n",
      "Processing train X rows: 160000 ~ 170000\n",
      "Processing train X rows: 170000 ~ 180000\n",
      "Processing train X rows: 180000 ~ 190000\n",
      "Processing train X rows: 190000 ~ 200000\n",
      "Processing train X rows: 200000 ~ 210000\n",
      "Processing train X rows: 210000 ~ 220000\n",
      "Processing train X rows: 220000 ~ 230000\n",
      "Processing train X rows: 230000 ~ 240000\n",
      "Processing train X rows: 240000 ~ 250000\n",
      "Processing train X rows: 250000 ~ 260000\n",
      "Processing train X rows: 260000 ~ 270000\n",
      "Processing train X rows: 270000 ~ 280000\n",
      "Processing train X rows: 280000 ~ 290000\n",
      "Processing test X rows: 0 ~ 10000\n",
      "Processing test X rows: 10000 ~ 20000\n",
      "Processing test X rows: 20000 ~ 30000\n",
      "Processing test X rows: 30000 ~ 40000\n",
      "Processing test X rows: 40000 ~ 50000\n",
      "Processing test X rows: 50000 ~ 60000\n",
      "Processing test X rows: 60000 ~ 70000\n",
      "Processing test X rows: 70000 ~ 80000\n",
      "Processing test X rows: 80000 ~ 90000\n",
      "Processing test X rows: 90000 ~ 100000\n",
      "['./Data Exploration/data/all_hosp_data_X_train_0.pkl', './Data Exploration/data/all_hosp_data_X_train_1.pkl', './Data Exploration/data/all_hosp_data_X_train_2.pkl', './Data Exploration/data/all_hosp_data_X_train_3.pkl', './Data Exploration/data/all_hosp_data_X_train_4.pkl', './Data Exploration/data/all_hosp_data_X_train_5.pkl', './Data Exploration/data/all_hosp_data_X_train_6.pkl', './Data Exploration/data/all_hosp_data_X_train_7.pkl', './Data Exploration/data/all_hosp_data_X_train_8.pkl', './Data Exploration/data/all_hosp_data_X_train_9.pkl', './Data Exploration/data/all_hosp_data_X_train_10.pkl', './Data Exploration/data/all_hosp_data_X_train_11.pkl', './Data Exploration/data/all_hosp_data_X_train_12.pkl', './Data Exploration/data/all_hosp_data_X_train_13.pkl', './Data Exploration/data/all_hosp_data_X_train_14.pkl', './Data Exploration/data/all_hosp_data_X_train_15.pkl', './Data Exploration/data/all_hosp_data_X_train_16.pkl', './Data Exploration/data/all_hosp_data_X_train_17.pkl', './Data Exploration/data/all_hosp_data_X_train_18.pkl', './Data Exploration/data/all_hosp_data_X_train_19.pkl', './Data Exploration/data/all_hosp_data_X_train_20.pkl', './Data Exploration/data/all_hosp_data_X_train_21.pkl', './Data Exploration/data/all_hosp_data_X_train_22.pkl', './Data Exploration/data/all_hosp_data_X_train_23.pkl', './Data Exploration/data/all_hosp_data_X_train_24.pkl', './Data Exploration/data/all_hosp_data_X_train_25.pkl', './Data Exploration/data/all_hosp_data_X_train_26.pkl', './Data Exploration/data/all_hosp_data_X_train_27.pkl', './Data Exploration/data/all_hosp_data_X_train_28.pkl'] ./Data Exploration/data/all_hosp_data_y_train.pkl ['./Data Exploration/data/all_hosp_data_X_test_0.pkl', './Data Exploration/data/all_hosp_data_X_test_1.pkl', './Data Exploration/data/all_hosp_data_X_test_2.pkl', './Data Exploration/data/all_hosp_data_X_test_3.pkl', './Data Exploration/data/all_hosp_data_X_test_4.pkl', './Data Exploration/data/all_hosp_data_X_test_5.pkl', './Data Exploration/data/all_hosp_data_X_test_6.pkl', './Data Exploration/data/all_hosp_data_X_test_7.pkl', './Data Exploration/data/all_hosp_data_X_test_8.pkl', './Data Exploration/data/all_hosp_data_X_test_9.pkl'] ./Data Exploration/data/all_hosp_data_y_test.pkl\n"
     ]
    }
   ],
   "source": [
    "X_train_file_paths, y_train_file_path, X_test_file_paths, y_test_file_path = dpl.transform_train_test_data(proc_data_path)\n",
    "print(X_train_file_paths, y_train_file_path, X_test_file_paths, y_test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 82.8 GiB for an array with shape (38885, 285908) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fd85948d9f8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m df_X_train = pd.concat([pd.read_pickle(file_path)\n\u001b[1;32m----> 2\u001b[1;33m                         for file_path in X_train_file_paths])\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'X_train {df_X_train.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_y_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    285\u001b[0m     )\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m--> 503\u001b[1;33m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             )\n\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_extension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;31m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\prpms\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py\u001b[0m in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis)\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[0mto_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"object\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 82.8 GiB for an array with shape (38885, 285908) and data type float64"
     ]
    }
   ],
   "source": [
    "df_X_train = pd.concat([pd.read_pickle(file_path)\n",
    "                        for file_path in X_train_file_paths])\n",
    "print(f'X_train {df_X_train.shape}')\n",
    "    \n",
    "df_y_train = pd.read_pickle(y_train_file_path)\n",
    "print(f'y_train {df_y_train.shape}')\n",
    "\n",
    "df_X_test = pd.concat([pd.read_pickle(file_path)\n",
    "                       for file_path in X_test_file_paths])\n",
    "print(f'X_test {df_X_test.shape}')\n",
    "\n",
    "df_y_test = pd.read_pickle(y_test_file_path)\n",
    "print(f'y_test {df_y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "impt_threshold = 0.0\n",
    "\n",
    "save_folder_paths = os.path.dirname(X_train_file_paths[0])\n",
    "    \n",
    "# forest\n",
    "forest = RandomForestRegressor(random_state=42)\n",
    "forest.fit(df_X_train, df_y_train)\n",
    "    \n",
    "forest_pkl_file_path = f'{save_folder_paths}/all_hosp_forest_model_uc1.pkl'\n",
    "with open(forest_pkl_file_path, 'wb') as file:\n",
    "    pickle.dump(forest, file)\n",
    "\n",
    "# feature importance\n",
    "print(f'{sum(forest.feature_importances_ > impt_threshold)} / {len(forest.feature_importances_)}')\n",
    "feature_importance_file_path = f'{save_folder_paths}/all_hosp_forest_feat_impt_uc1.npy'\n",
    "np.save(feature_importance_file_path, forest.feature_importances_ > impt_threshold)\n",
    "\n",
    "# new training files\n",
    "df_X_train_new = df_X_train.loc[:, forest.feature_importances_ > impt_threshold]\n",
    "print(df_X_train_new.shape)\n",
    "new_X_train_pkl_file_path = f'{save_dir}/all_hosp_new_train_X_uc1.pkl'\n",
    "df_X_train_new.to_pickle(new_X_train_pkl_file_path)\n",
    "\n",
    "# new test files\n",
    "df_X_test_new = df_X_test.loc[:, forest.feature_importances_ > impt_threshold]\n",
    "print(df_X_test_new.shape)\n",
    "new_X_test_pkl_file_path = f'{save_dir}/all_hosp_new_test_X_uc1.pkl'\n",
    "df_X_test_new.to_pickle(new_X_test_pkl_file_path)\n",
    "\n",
    "print('forest:\\n', forest_pkl_file_path)\n",
    "print('feature importance:\\n', feature_importance_file_path)\n",
    "print('New X_train:\\n', new_X_train_pkl_file_path)\n",
    "print('New X_test:\\n', new_X_test_pkl_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(df):\n",
    "    #df1 = dpl.bin_column(df.to_frame(), col='WRITE_OFF', bin_thresh = [500])\n",
    "    #df1 = pd.get_dummies(df1['bin_WRITE_OFF'])\n",
    "    #df1 = df1.drop(0, axis=1)\n",
    "    #df1.columns = ['WRITE_OFF_LABEL']\n",
    "    df1 = df.copy()\n",
    "    df1['WRITE_OFF_LABEL'] = ((df1['WRITE_OFF'] > 500) | (df1['WRITE_OFF'] > 0.05 * df1['TOTAL_FEES'])).astype(int)\n",
    "    df1.drop(columns=['WRITE_OFF'], inplace=True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pr.read_excel(raw_data_path)\n",
    "print(convert_y(df_test)['WRITE_OFF_LABEL'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_train_new = convert_y(df_y_train)\n",
    "df_y_test_new = convert_y(df_y_test)\n",
    "print(df_y_train_new['WRITE_OFF_LABEL'].value_counts())\n",
    "print(df_y_test_new['WRITE_OFF_LABEL'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree WITHOUT SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = 'entropy'\n",
    "rand_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_cls = DecisionTreeClassifier(criterion=criterion,random_state=rand_seed)\n",
    "dt_cls.fit(df_X_train_new, df_y_train_new)\n",
    "print(dt_cls)\n",
    "print(f\"Accuracy on training set without SMOTE decision tree: {dt_cls.score(df_X_train_new, df_y_train_new)}\")\n",
    "print(f\"Accuracy on test set without SMOTE decision tree: {dt_cls.score(df_X_test_new, df_y_test_new)}\")\n",
    "\n",
    "filename = file_dir + 'all_hosp_uc1_no_smote_dt_model.pkl'\n",
    "pickle.dump(dt_cls, open(filename, 'wb'))\n",
    "\n",
    "y_pred = dt_cls.predict(df_X_test_new)\n",
    "y_pred.to_pickle(file_dir + 'all_hosp_uc1_no_smote_predictions.pkl')\n",
    "print('no SMOTE decision tree \\n', confusion_matrix(df_y_test_new, y_pred))  \n",
    "print('no SMOTE decision tree \\n', classification_report(df_y_test_new, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree WITH SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree WITH SMOTE SS = 0.1 ~ 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in np.arange(0.1, 1.1, 0.1):\n",
    "    sm_ss = SMOTE(random_state=55,sampling_strategy=sample)\n",
    "    X_train_ss, y_train_ss = sm_ss.fit_sample(df_X_train_new, df_y_train_new)\n",
    "\n",
    "    model = DecisionTreeClassifier(criterion=criterion,random_state=rand_seed)\n",
    "    model.fit(X_train_ss, y_train_ss)\n",
    "    print(model)\n",
    "    print(f\"Accuracy on training set with {sample} SMOTE decision tree: {model.score(X_train_ss, y_train_ss)}\")\n",
    "    print(f\"Accuracy on test set with {sample} SMOTE decision tree: {model.score(df_X_test_new, df_y_test_new)}\")\n",
    "\n",
    "    filename = file_dir + f'all_hosp_uc1_{sample}_smote_dt_model.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "    y_pred = model.predict(df_X_test_new)\n",
    "    print(f'{sample} SMOTE decision tree \\n', confusion_matrix(df_y_test_new, y_pred))  \n",
    "    print(f'{sample} SMOTE decision tree \\n', classification_report(df_y_test_new, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_strength = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=reg_strength).fit(df_X_train_new, df_y_train_new)\n",
    "print(logreg)\n",
    "print(f\"Training set without SMOTE log reg score: {logreg.score(df_X_train_new, df_y_train_new)}\")\n",
    "print(f\"Test set without SMOTE log reg score: {logreg.score(df_X_test_new, df_y_test_new)}\")\n",
    "\n",
    "filename = file_dir + 'all_hosp_uc1_no_smote_logreg_model.pkl'\n",
    "pickle.dump(logreg, open(filename, 'wb'))\n",
    "\n",
    "y_pred = logreg.predict(df_X_test_new)\n",
    "print('no SMOTE log reg \\n', confusion_matrix(df_y_test_new, y_pred))  \n",
    "print('no SMOTE log reg \\n', classification_report(df_y_test_new, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression With SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression With SMOTE ss=0.1 ~ 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in np.arange(0.1, 1.1, 0.1):\n",
    "    sm_ss = SMOTE(random_state=55,sampling_strategy=sample)\n",
    "    X_train_ss, y_train_ss = sm_ss.fit_sample(df_X_train_new, df_y_train_new)\n",
    "\n",
    "    model = LogisticRegression(C=reg_strength).fit(X_train_ss, y_train_ss)\n",
    "    print(model)\n",
    "    print(f\"Training set with {sample} SMOTE log reg score: {model.score(X_train_ss, y_train_ss)}\")\n",
    "    print(f\"Test set with {sample} SMOTE log reg score: {model.score(df_X_test_new, df_y_test_new)}\")\n",
    "\n",
    "    filename = file_dir + f'all_hosp_uc1_{sample}_smote_logreg_model.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "    y_pred = model.predict(df_X_test_new)\n",
    "    print(f'{sample} SMOTE log reg \\n', confusion_matrix(df_y_test_new, y_pred))  \n",
    "    print(f'{sample} SMOTE log reg \\n', classification_report(df_y_test_new, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the Gaussian Classifier\n",
    "gaussnb = GaussianNB()\n",
    "\n",
    "# Training your model \n",
    "gaussnb.fit(df_X_train_new, df_y_train_new)\n",
    "print(gaussnb)\n",
    "filename = file_dir + 'all_hosp_uc1_no_smote_nb_model.pkl'\n",
    "pickle.dump(gaussnb, open(filename, 'wb'))\n",
    "\n",
    "# Score\n",
    "print(f\"Training set without SMOTE Naive Bayes score: {gaussnb.score(df_X_train_new, df_y_train_new)}\")\n",
    "print(f\"Test set without SMOTE Naive Bayes score: {gaussnb.score(df_X_test_new, df_y_test_new)}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred = gaussnb.predict(df_X_test_new)\n",
    "print('no SMOTE Naive Bayes \\n', confusion_matrix(df_y_test_new, y_pred))  \n",
    "print('no SMOTE Naive Bayes \\n', classification_report(df_y_test_new, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes With SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Without SMOTE ss=0.1 ~ 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in np.arange(0.1, 1.1, 0.1):\n",
    "    sm_ss = SMOTE(random_state=55,sampling_strategy=sample)\n",
    "    X_train_ss, y_train_ss = sm_ss.fit_sample(df_X_train_new, df_y_train_new)\n",
    "\n",
    "    # Initiating the Gaussian Classifier\n",
    "    model = GaussianNB()\n",
    "\n",
    "    # Training your model \n",
    "    model.fit(X_train_ss, y_train_ss)\n",
    "    print(model)\n",
    "    filename = file_dir + f'all_hosp_uc1_{sample}_smote_nb_model.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "    # Score\n",
    "    print(f\"Training set with {sample} SMOTE Naive Bayes score: {model.score(X_train_ss, y_train_ss)}\")\n",
    "    print(f\"Test set with {sample} SMOTE Naive Bayes score: {model.score(df_X_test_new, df_y_test_new)}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    y_pred = model.predict(df_X_test_new)\n",
    "    print(f'{sample} Naive Bayes\\n', confusion_matrix(df_y_test_new, y_pred))  \n",
    "    print(f'{sample} Naive Bayes\\n', classification_report(df_y_test_new, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURAL NET MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net MLP Without SMOTE, 2 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hid_layers = [(128,128), (16,16,16), (32,32,32), (16,16,16,16), (32,32,32,32)]\n",
    "max_iter = 1000\n",
    "list_smote_sampling = np.arange(0.1, 1.1, 0.1)#[0.1, 0.2, 0.3, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Net MLPClassifier without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hid_layers in list_hid_layers:\n",
    "    model = MLPClassifier(hidden_layer_sizes=hid_layers, max_iter=max_iter,verbose=1)  \n",
    "    model.fit(df_X_train_new, df_y_train_new)\n",
    "    print(model)\n",
    "    filename = file_dir + f'all_hosp_uc1_no_smote_mlp_{hid_layers}_model.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "    predictions = mlp.predict(df_X_test_new) \n",
    "\n",
    "    # Score\n",
    "    print(f\"Training set without SMOTE mlp {hid_layers} score: {model.score(df_X_train_new, df_y_train_new)}\")\n",
    "    print(f\"Test set without SMOTE mlp {hid_layers} score: {model.score(df_X_test_new, df_y_test_new)}\")\n",
    "\n",
    "    print(f\"Accuracy without SMOTE mlp {hid_layers}: \", metrics.accuracy_score(df_y_test_new, predictions))\n",
    "    print(f'no SMOTE mlp {hid_layers}\\n', confusion_matrix(df_y_test_new,predictions))  \n",
    "    print(f'no SMOTE mlp {hid_layers}\\n', classification_report(df_y_test_new,predictions))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hid_layers = [(16,16), (32,32), (64,64), (128,128), (256,256),\n",
    "                   (16,16,16), (32,32,32), (64,64,64),\n",
    "                   (16,16,16,16), (32,32,32,32)]\n",
    "max_iter = 1000\n",
    "list_smote_sampling = np.arange(0.1, 1.1, 0.1)#[0.1, 0.2, 0.3, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Net Layers With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hid_layers in list_hid_layers:\n",
    "    for sample in list_smote_sampling:\n",
    "        sm_ss = SMOTE(random_state=55,sampling_strategy=sample)\n",
    "        X_train_ss, y_train_ss = sm_ss.fit_sample(df_X_train_new, df_y_train_new)\n",
    "\n",
    "        model = MLPClassifier(hidden_layer_sizes=hid_layers, max_iter=max_iter,verbose=1)  \n",
    "        model.fit(X_train_ss, y_train_ss)  \n",
    "        print(model)\n",
    "        filename = file_dir + f'all_hosp_uc1_{sample}_smote_mlp_{hid_layers}_model.pkl'\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "        predictions = model.predict(df_X_test_new) \n",
    "\n",
    "        # Score\n",
    "        print(f\"Training set {sample} SMOTE mlp {hid_layers} score: {model.score(X_train_ss, y_train_ss)}\")\n",
    "        print(f\"Test set {sample} SMOTE mlp {hid_layers} score: {model.score(df_X_test_new, df_y_test_new)}\")\n",
    "\n",
    "        print(f\"Accuracy: \", metrics.accuracy_score(df_y_test_new, predictions))\n",
    "        print(f'{sample} SMOTE mlp {hid_layers}\\n', confusion_matrix(df_y_test_new,predictions))  \n",
    "        print(f'{sample} SMOTE mlp {hid_layers}\\n', classification_report(df_y_test_new,predictions))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# From the above, the best KNN is at k=8\n",
    "#Setup a knn classifier with k neighbors\n",
    "best_knn = KNeighborsClassifier(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the KNN model with training data and score the test data\n",
    "best_knn = KNeighborsClassifier(n_neighbours)\n",
    "\n",
    "best_knn.fit(df_X_train_new,df_y_train_new)\n",
    "best_knn.score(df_X_test_new,df_y_test_new)\n",
    "\n",
    "# save the model to disk\n",
    "filename = f'all_hosp_knn_without_smote.pkl'\n",
    "pickle.dump(best_knn, open(filename, 'wb'))\n",
    "\n",
    "y_pred = best_knn.predict(df_X_test_new)\n",
    "# Score for Best KNN, No SMOTE\n",
    "print(f\"Training set without SMOTE KNN({n_neighbours}) score: {best_knn.score(df_X_train_new, df_y_train_new)}\")\n",
    "print(f\"Test set without SMOTE KNN({n_neighbours}) score: {best_knn.score(df_X_test_new, df_y_test_new)}\")\n",
    "\n",
    "print(f\"Accuracy without SMOTE KNN({n_neighbours}): {metrics.accuracy_score(df_y_test_new, y_pred)}\")\n",
    "print(f'no SMOTE KNN({n_neighbours})\\n', confusion_matrix(df_y_test_new,y_pred))  \n",
    "print(f'no SMOTE KNN({n_neighbours})\\n', classification_report(df_y_test_new,y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN With SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN With SMOTE ss=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_smote_sampling = np.arange(0.1, 1.1, 0.1)#[0.1, 0.5, 1.0]\n",
    "for sample in list_smote_sampling:\n",
    "    sm_ss = SMOTE(random_state=55,sampling_strategy=sample)\n",
    "    X_train_ss, y_train_ss = sm_ss.fit_sample(df_X_train_new, df_y_train_new)\n",
    "\n",
    "    # Fit the KNN model with training data and score the test data\n",
    "    model = KNeighborsClassifier(n_neighbours)    \n",
    "    model.fit(X_train_ss,y_train_ss)\n",
    "    #model.score(df_X_test_new,df_y_test_new)\n",
    "\n",
    "    # save the model to disk\n",
    "    filename = f'all_hosp_knn{n_neighbours}_{sample}_smote.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "    y_pred = model.predict(df_X_test_new)\n",
    "    # Score for Best KNN, with SMOTE\n",
    "    print(f\"Training set {sample} SMOTE KNN({n_neighbours}) score: {model.score(X_train_ss, y_train_ss)}\")\n",
    "    print(f\"Test set {sample} SMOTE KNN({n_neighbours}) score: {model.score(df_X_test_new, df_y_test_new)}\")\n",
    "\n",
    "    print(f\"Accuracy {sample} SMOTE KNN({n_neighbours}): \", metrics.accuracy_score(df_y_test_new, y_pred))\n",
    "    print(f'{sample} SMOTE KNN({n_neighbours})\\n', confusion_matrix(df_y_test_new,y_pred))  \n",
    "    print(f'{sample} SMOTE KNN({n_neighbours})\\n', classification_report(df_y_test_new,y_pred)) "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
